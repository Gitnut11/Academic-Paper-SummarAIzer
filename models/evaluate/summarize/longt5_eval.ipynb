{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T15:37:58.295812Z",
     "iopub.status.busy": "2025-04-30T15:37:58.295529Z",
     "iopub.status.idle": "2025-04-30T15:39:53.277622Z",
     "shell.execute_reply": "2025-04-30T15:39:53.276384Z",
     "shell.execute_reply.started": "2025-04-30T15:37:58.295785Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "!pip install -q triton bitsandbytes accelerate hf_xet\n",
    "!pip install -q evaluate bert_score rouge_score git+https://github.com/google-research/bleurt.git\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T15:39:53.280906Z",
     "iopub.status.busy": "2025-04-30T15:39:53.280556Z",
     "iopub.status.idle": "2025-04-30T15:40:29.523693Z",
     "shell.execute_reply": "2025-04-30T15:40:29.522703Z",
     "shell.execute_reply.started": "2025-04-30T15:39:53.280872Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from peft import PeftModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T15:40:29.525507Z",
     "iopub.status.busy": "2025-04-30T15:40:29.524845Z",
     "iopub.status.idle": "2025-04-30T15:40:30.355814Z",
     "shell.execute_reply": "2025-04-30T15:40:30.354811Z",
     "shell.execute_reply.started": "2025-04-30T15:40:29.525482Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T15:40:30.357111Z",
     "iopub.status.busy": "2025-04-30T15:40:30.356799Z",
     "iopub.status.idle": "2025-04-30T15:40:30.496920Z",
     "shell.execute_reply": "2025-04-30T15:40:30.496099Z",
     "shell.execute_reply.started": "2025-04-30T15:40:30.357081Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "BASE_MODEL = \"google/long-t5-tglobal-base\"\n",
    "REPO_NAME = \"Mels22/longt5-scisummnet\"\n",
    "DATA_CSV = \"/kaggle/input/scisummnet-corpus/scisumm.csv\"\n",
    "\n",
    "CHUNK_SIZE = 8192\n",
    "OVERLAP_SIZE = 512\n",
    "MAX_TARGET_LENGTH = 512\n",
    "\n",
    "BATCH_SIZE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T15:40:30.498461Z",
     "iopub.status.busy": "2025-04-30T15:40:30.498064Z",
     "iopub.status.idle": "2025-04-30T15:40:30.509561Z",
     "shell.execute_reply": "2025-04-30T15:40:30.508567Z",
     "shell.execute_reply.started": "2025-04-30T15:40:30.498431Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ScisummnetDataset:\n",
    "    def __init__(self, path, tokenizer, chunk_size=CHUNK_SIZE, overlap=OVERLAP_SIZE):\n",
    "        df = pd.read_csv(path)\n",
    "        self.hf_dataset = Dataset.from_pandas(df)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.chunk_size = chunk_size\n",
    "        self.overlap = overlap\n",
    "\n",
    "    def _process_data_to_model_inputs(self, batch):\n",
    "        all_input_ids = []\n",
    "        all_attention_masks = []\n",
    "        all_labels = []\n",
    "\n",
    "        for text, summary in zip(batch[\"text\"], batch[\"summary\"]):\n",
    "            tokenized_inputs = self.tokenizer(\n",
    "                text,\n",
    "                return_overflowing_tokens=True,\n",
    "                stride=self.overlap,\n",
    "                truncation=True,\n",
    "                max_length=self.chunk_size,\n",
    "                padding=\"max_length\",\n",
    "            )\n",
    "\n",
    "            tokenized_outputs = self.tokenizer(\n",
    "                summary,\n",
    "                truncation=True,\n",
    "                max_length=MAX_TARGET_LENGTH,\n",
    "                padding=\"max_length\",\n",
    "            )\n",
    "\n",
    "            for input_ids, attention_mask in zip(\n",
    "                tokenized_inputs[\"input_ids\"], tokenized_inputs[\"attention_mask\"]\n",
    "            ):\n",
    "                # Apply -100 masking to pad tokens in the label\n",
    "                labels = [\n",
    "                    -100 if token == self.tokenizer.pad_token_id else token\n",
    "                    for token in tokenized_outputs[\"input_ids\"]\n",
    "                ]\n",
    "\n",
    "                all_input_ids.append(input_ids)\n",
    "                all_attention_masks.append(attention_mask)\n",
    "                all_labels.append(labels)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": np.array(all_input_ids, dtype=np.int64),\n",
    "            \"attention_mask\": np.array(all_attention_masks, dtype=np.int64),\n",
    "            \"labels\": np.array(all_labels, dtype=np.int64),\n",
    "        }\n",
    "\n",
    "    def get_data(self, test_size=0.1):\n",
    "        split_data = self.hf_dataset.train_test_split(test_size=test_size)\n",
    "        train_ds = split_data[\"train\"]\n",
    "        val_ds = split_data[\"test\"]\n",
    "\n",
    "        train_data = train_ds.map(\n",
    "            self._process_data_to_model_inputs,\n",
    "            batched=True,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            remove_columns=[\"text\", \"summary\"],\n",
    "        )\n",
    "        train_data.set_format(\n",
    "            type=\"torch\",\n",
    "            columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "            output_all_columns=False,  # make sure only required tensors are kept\n",
    "        )\n",
    "\n",
    "        val_data = val_ds.map(\n",
    "            self._process_data_to_model_inputs,\n",
    "            batched=True,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            remove_columns=[\"text\", \"summary\"],\n",
    "        )\n",
    "        val_data.set_format(\n",
    "            type=\"torch\",\n",
    "            columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "            output_all_columns=False,  # make sure only required tensors are kept\n",
    "        )\n",
    "\n",
    "        return {\"train\": train_data, \"val\": val_data}, val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T15:40:30.535928Z",
     "iopub.status.busy": "2025-04-30T15:40:30.535613Z",
     "iopub.status.idle": "2025-04-30T15:40:30.561321Z",
     "shell.execute_reply": "2025-04-30T15:40:30.560339Z",
     "shell.execute_reply.started": "2025-04-30T15:40:30.535903Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class LongT5Inference:\n",
    "    def __init__(\n",
    "        self,\n",
    "        chunk_size=CHUNK_SIZE,\n",
    "        overlap=OVERLAP_SIZE,\n",
    "        max_gen_len=MAX_TARGET_LENGTH,\n",
    "        batch_size=4,  # New param: safe batch size\n",
    "    ):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = overlap\n",
    "        self.max_len = max_gen_len\n",
    "        self.batch_size = batch_size\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        base_model = AutoModelForSeq2SeqLM.from_pretrained(BASE_MODEL)\n",
    "        self.model = PeftModel.from_pretrained(base_model, REPO_NAME)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        self.rouge = self.bert_score = self.bleurt = self.meteor = None\n",
    "\n",
    "    def _semantic_split(self, text):\n",
    "        sentences = sent_tokenize(text)\n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        current_length = 0\n",
    "\n",
    "        for sentence in sentences:\n",
    "            sentence_tokens = self.tokenizer.tokenize(sentence)\n",
    "            sentence_length = len(sentence_tokens)\n",
    "\n",
    "            if current_length + sentence_length > self.chunk_size:\n",
    "                chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "                if self.chunk_overlap > 0:\n",
    "                    overlap_sentences = []\n",
    "                    total_overlap = 0\n",
    "                    for prev_sentence in reversed(current_chunk):\n",
    "                        tokens = self.tokenizer.tokenize(prev_sentence)\n",
    "                        total_overlap += len(tokens)\n",
    "                        overlap_sentences.insert(0, prev_sentence)\n",
    "                        if total_overlap >= self.chunk_overlap:\n",
    "                            break\n",
    "                    current_chunk = overlap_sentences + [sentence]\n",
    "                    current_length = total_overlap + sentence_length\n",
    "                else:\n",
    "                    current_chunk = [sentence]\n",
    "                    current_length = sentence_length\n",
    "            else:\n",
    "                current_chunk.append(sentence)\n",
    "                current_length += sentence_length\n",
    "\n",
    "        if current_chunk:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    def _batch_summarize(self, texts):\n",
    "        summaries = []\n",
    "        for i in range(0, len(texts), self.batch_size):\n",
    "            batch = texts[i : i + self.batch_size]\n",
    "\n",
    "            inputs = self.tokenizer(\n",
    "                batch,\n",
    "                truncation=True,\n",
    "                max_length=self.chunk_size,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\",\n",
    "            ).to(self.device)\n",
    "\n",
    "            global_attention_mask = torch.zeros_like(inputs[\"attention_mask\"]).to(\n",
    "                self.device\n",
    "            )\n",
    "            global_attention_mask[:, 0] = 1\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    input_ids=inputs[\"input_ids\"],\n",
    "                    attention_mask=inputs[\"attention_mask\"],\n",
    "                    max_new_tokens=self.max_len,\n",
    "                    do_sample=False,\n",
    "                    num_beams=2,\n",
    "                    no_repeat_ngram_size=3,\n",
    "                    repetition_penalty=2.0,\n",
    "                    length_penalty=1.0,\n",
    "                    early_stopping=True,\n",
    "                )\n",
    "\n",
    "            batch_summaries = self.tokenizer.batch_decode(\n",
    "                outputs, skip_special_tokens=True\n",
    "            )\n",
    "            summaries.extend([s.strip() for s in batch_summaries])\n",
    "\n",
    "            del inputs, global_attention_mask, outputs\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        return summaries\n",
    "\n",
    "    def infer(self, text):\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        tokenized_text = self.tokenizer.tokenize(text)\n",
    "        if len(tokenized_text) <= self.chunk_size:\n",
    "            # Direct summarization\n",
    "            return self._batch_summarize([text])[0]\n",
    "\n",
    "        # Otherwise, semantic split\n",
    "        chunks = self._semantic_split(text)\n",
    "\n",
    "        # Batch summarize chunks\n",
    "        chunk_summaries = self._batch_summarize(chunks)\n",
    "\n",
    "        combined_summary = \" \".join(chunk_summaries)\n",
    "\n",
    "        # Final summarization\n",
    "        final_summary = self._batch_summarize([combined_summary])[0]\n",
    "\n",
    "        return final_summary\n",
    "\n",
    "    def _load_metrics(self, metric_keys):\n",
    "        if \"rouge\" in metric_keys and self.rouge is None:\n",
    "            self.rouge = evaluate.load(\"rouge\")\n",
    "        if \"bertscore\" in metric_keys and self.bert_score is None:\n",
    "            self.bert_score = evaluate.load(\"bertscore\")\n",
    "        if \"bleurt\" in metric_keys and self.bleurt is None:\n",
    "            self.bleurt = evaluate.load(\n",
    "                \"bleurt\",\n",
    "                module_type=\"metric\",\n",
    "            )\n",
    "        if \"meteor\" in metric_keys and self.meteor is None:\n",
    "            self.meteor = evaluate.load(\"meteor\")\n",
    "\n",
    "    def evaluate(self, dataset, metric_keys=[\"all\"]):\n",
    "        if metric_keys is None or metric_keys == [\"none\"] or metric_keys == \"none\":\n",
    "            raise Exception(\n",
    "                \"⚠️ Please define the metric to calculate on: `rouge`, `bertscore`, `bleurt`, `meteor`, `time`, or `all`\"\n",
    "            )\n",
    "        if isinstance(metric_keys, str):\n",
    "            metric_keys = [metric_keys]\n",
    "        if \"all\" in metric_keys:\n",
    "            metric_keys = [\"rouge\", \"bertscore\", \"bleurt\", \"meteor\", \"time\"]\n",
    "        self._load_metrics(metric_keys)\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        predictions, references = [], []\n",
    "        total_time = 0.0\n",
    "        for record in tqdm(dataset, desc=\"Evaluate\"):\n",
    "            references.append(record[\"summary\"])\n",
    "            start_time = time.time()\n",
    "            predictions.append(self.infer(record[\"text\"]))\n",
    "            total_time += time.time() - start_time\n",
    "\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        results = {}\n",
    "        if \"rouge\" in metric_keys:\n",
    "            rouge_result = self.rouge.compute(\n",
    "                predictions=predictions, references=references, use_stemmer=True\n",
    "            )\n",
    "            results.update({f\"rouge_{k}\": v for k, v in rouge_result.items()})\n",
    "        if \"bertscore\" in metric_keys:\n",
    "            bert_result = self.bert_score.compute(\n",
    "                predictions=predictions, references=references, lang=\"en\"\n",
    "            )\n",
    "            results.update(\n",
    "                {\n",
    "                    f\"bertscore_{k}\": sum(v) / len(v)\n",
    "                    for k, v in bert_result.items()\n",
    "                    if isinstance(v, list)\n",
    "                }\n",
    "            )\n",
    "        if \"bleurt\" in metric_keys:\n",
    "            bleurt_result = self.bleurt.compute(\n",
    "                predictions=predictions, references=references\n",
    "            )\n",
    "            results.update(\n",
    "                {f\"bleurt\": sum(bleurt_result[\"scores\"]) / len(bleurt_result[\"scores\"])}\n",
    "            )\n",
    "        if \"meteor\" in metric_keys:\n",
    "            meteor_result = self.meteor.compute(\n",
    "                predictions=predictions, references=references\n",
    "            )\n",
    "            results.update(meteor_result)\n",
    "        if \"time\" in metric_keys:\n",
    "            results.update({\"avg_inference_in_sec\": total_time / len(dataset)})\n",
    "        del predictions, references\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T15:40:30.616757Z",
     "iopub.status.busy": "2025-04-30T15:40:30.616469Z",
     "iopub.status.idle": "2025-04-30T15:40:41.095991Z",
     "shell.execute_reply": "2025-04-30T15:40:41.094752Z",
     "shell.execute_reply.started": "2025-04-30T15:40:30.616737Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "longt5 = LongT5Inference(batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T15:40:41.097869Z",
     "iopub.status.busy": "2025-04-30T15:40:41.097271Z",
     "iopub.status.idle": "2025-04-30T15:41:26.647058Z",
     "shell.execute_reply": "2025-04-30T15:41:26.646115Z",
     "shell.execute_reply.started": "2025-04-30T15:40:41.097836Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "scisummnet = ScisummnetDataset(DATA_CSV, longt5.tokenizer)\n",
    "data_loader, val_df = scisummnet.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T15:41:26.648578Z",
     "iopub.status.busy": "2025-04-30T15:41:26.648248Z",
     "iopub.status.idle": "2025-04-30T15:48:08.623110Z",
     "shell.execute_reply": "2025-04-30T15:48:08.622137Z",
     "shell.execute_reply.started": "2025-04-30T15:41:26.648553Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "scientific_paragraph = \"\"\"\n",
    "The vast and intricate field of neuroscience, dedicated to unraveling the mysteries of the nervous system, encompasses a multitude of disciplines, each contributing unique insights into the complexities of brain structure, function, and development. From the molecular level, where neurochemists explore the synthesis, release, and action of neurotransmitters, to the systems level, where neurophysiologists investigate the coordinated activity of neural circuits, neuroscience seeks to understand how the brain gives rise to behavior, cognition, and consciousness. The fundamental building block of the nervous system, the neuron, is a specialized cell characterized by its unique morphology, featuring a cell body (soma), dendrites that receive incoming signals, and an axon that transmits signals to other neurons or effector cells. The electrical signaling within neurons relies on the generation and propagation of action potentials, rapid changes in membrane potential driven by the precisely regulated opening and closing of ion channels, primarily voltage-gated sodium and potassium channels. These action potentials travel along the axon, and at the axon terminal, they trigger the release of neurotransmitters into the synaptic cleft, the narrow gap between neurons. Neurotransmitters, a diverse group of chemical messengers including acetylcholine, dopamine, serotonin, glutamate, and GABA, bind to specific receptors on the postsynaptic neuron, initiating a cascade of intracellular signaling events that can either excite or inhibit the postsynaptic neuron, thereby influencing its likelihood of firing an action potential. The precise balance of excitatory and inhibitory synaptic transmission is crucial for proper brain function, and disruptions in this balance have been implicated in a variety of neurological and psychiatric disorders. Synaptic plasticity, the ability of synapses to strengthen or weaken over time in response to changes in neural activity, is a fundamental mechanism underlying learning and memory. Long-term potentiation (LTP), a form of synaptic plasticity characterized by a persistent increase in synaptic strength following high-frequency stimulation, is widely studied as a cellular model of learning. Conversely, long-term depression (LTD), a decrease in synaptic strength following low-frequency stimulation, is thought to contribute to the forgetting of irrelevant information. The brain is organized into distinct anatomical regions, each with specialized functions. The cerebral cortex, the outermost layer of the brain, is responsible for higher-order cognitive functions such as language, memory, and reasoning. The cortex is divided into four lobes: the frontal lobe, involved in planning, decision-making, and motor control; the parietal lobe, responsible for sensory perception, spatial awareness, and attention; the temporal lobe, crucial for auditory processing, memory, and language comprehension; and the occipital lobe, dedicated to visual processing. Within each lobe, specific cortical areas are organized into functional maps, reflecting the precise representation of sensory or motor information. For example, the primary visual cortex in the occipital lobe contains a retinotopic map, where neighboring neurons respond to neighboring regions of the visual field. The motor cortex in the frontal lobe contains a somatotopic map, where different body parts are represented in a specific spatial arrangement. The intricate connectivity of the brain is organized into complex neural networks, allowing for the integration and processing of information across different brain regions. These networks can be studied using a variety of techniques, including tractography, which uses diffusion tensor imaging (DTI) to visualize white matter tracts, and functional connectivity analysis, which examines the temporal correlations in neural activity between different brain regions. The default mode network (DMN), a network of brain regions that is most active when a person is at rest and not engaged in any specific task, has been implicated in self-referential thought and mind-wandering. The salience network, involved in detecting and responding to salient stimuli, plays a crucial role in attention and cognitive control. The central executive network, responsible for working memory and cognitive flexibility, is essential for goal-directed behavior. The development of the nervous system is a remarkably complex and precisely orchestrated process, beginning with the formation of the neural tube during embryogenesis. Neural tube development is influenced by a complex interplay of genetic factors and signaling molecules, including sonic hedgehog (Shh), Wnt, and fibroblast growth factors (FGFs). Neurons are generated in specific regions of the developing brain through a process called neurogenesis, and they migrate to their final destinations, guided by a variety of cues, including cell adhesion molecules and extracellular matrix proteins. Axons grow and navigate to their target cells, forming synapses and establishing functional neural circuits. The formation of synapses is a highly regulated process, involving the precise matching of pre- and postsynaptic partners and the expression of specific synaptic adhesion molecules. The developing brain exhibits a high degree of plasticity, allowing it to adapt to environmental influences and experiences. Critical periods, specific time windows during development when the brain is particularly sensitive to certain types of input, are essential for the proper development of sensory and motor systems. For example, the visual system undergoes a critical period during early childhood, during which visual experience is necessary for the development of normal visual acuity. The study of brain disorders provides valuable insights into the functions of the nervous system. Neurological disorders, such as Alzheimer's disease, Parkinson's disease, and stroke, result from damage to or dysfunction of specific brain regions or neural circuits. Alzheimer's disease, a progressive neurodegenerative disorder, is characterized by the accumulation of amyloid plaques and neurofibrillary tangles in the brain, leading to neuronal loss and cognitive decline. Parkinson's disease, another neurodegenerative disorder, is caused by the loss of dopamine-producing neurons in the substantia nigra, resulting in motor symptoms such as tremor, rigidity, and bradykinesia. Stroke, a sudden interruption of blood flow to the brain, can cause permanent brain damage and a variety of neurological deficits, depending on the affected brain region. Psychiatric disorders, such as depression, schizophrenia, and anxiety disorders, are complex mental health conditions that involve disruptions in brain function and neural circuitry. Depression is characterized by persistent sadness, loss of interest, and changes in sleep and appetite. Schizophrenia, a severe mental disorder, is associated with hallucinations, delusions, and disorganized thinking. Anxiety disorders, such as generalized anxiety disorder and panic disorder, involve excessive fear and worry. The treatment of neurological and psychiatric disorders often involves a combination of pharmacological interventions, which target specific neurotransmitter systems or signaling pathways, and behavioral therapies, which aim to modify maladaptive thought patterns and behaviors. Deep brain stimulation (DBS), a neurosurgical procedure that involves implanting electrodes in specific brain regions to deliver electrical stimulation, has shown promise in the treatment of Parkinson's disease, essential tremor, and other movement disorders. Transcranial magnetic stimulation (TMS), a non-invasive technique that uses magnetic fields to stimulate or inhibit neural activity, is being investigated as a potential treatment for depression, anxiety, and other psychiatric disorders. The ongoing development of new neurotechnologies is revolutionizing our ability to study and manipulate the brain. Optogenetics, a technique that uses light to control the activity of genetically modified neurons, allows for precise control of neural circuits and has provided valuable insights into the neural basis of behavior. CLARITY, a technique that renders brain tissue transparent, allows for the visualization of neural circuits in three dimensions. Brain-computer interfaces (BCIs), which allow for direct communication between the brain and external devices, hold promise for restoring function in individuals with paralysis and other neurological conditions. The ethical implications of these new neurotechnologies are a subject of ongoing debate and discussion, raising important questions about privacy, autonomy, and the potential for misuse. The future of neuroscience holds immense promise for advancing our understanding of the brain and developing new treatments for neurological and psychiatric disorders. Continued research into the molecular, cellular, and systems-level mechanisms of brain function, coupled with the development of innovative neurotechnologies, will undoubtedly lead to groundbreaking discoveries and transformative advances in the years to come. The study of consciousness, one of the most profound and challenging problems in neuroscience, continues to captivate researchers from various disciplines. Defining consciousness, identifying its neural correlates, and understanding its evolutionary origins remain fundamental questions. Various theories of consciousness have been proposed, including the global workspace theory, which posits that consciousness arises from the broadcasting of information across a global network of brain regions, and the integrated information theory, which proposes that consciousness is related to the complexity and integration of information within a system. The search for the neural correlates of consciousness (NCC), the minimal set of neural events sufficient for a conscious experience, is a central focus of research in this area. Studies using neuroimaging techniques, such as fMRI and EEG, have identified several brain regions and neural networks that are thought to play a role in consciousness, including the thalamocortical system, the frontoparietal network, and the insula. The role of attention in consciousness is also a subject of intense investigation. Attention, the process of selectively focusing on certain aspects of the environment while ignoring others, is closely linked to conscious awareness. It has been proposed that attention is necessary, but not sufficient, for consciousness. Further research is needed to fully elucidate the relationship between attention and consciousness. The evolution of consciousness is another intriguing area of study. It is thought that consciousness evolved gradually over millions of years, perhaps as a way to integrate information and make more complex decisions. Comparative studies of brain structure and function in different species can provide insights into the evolutionary origins of consciousness. The development of artificial consciousness is a long-term goal of artificial intelligence research. While current AI systems can perform many tasks that once required human intelligence, they do not possess the subjective experience of consciousness. Creating a truly conscious artificial intelligence would require a deep understanding of the neural mechanisms underlying consciousness. The ethical implications of artificial consciousness are profound, raising questions about the rights and moral status of conscious machines. The study of the mind-brain relationship, the philosophical problem of how mental states are related to physical states in the brain, remains a central challenge in neuroscience and philosophy. Dualism, the view that the mind and brain are separate entities, has been a long-standing philosophical position, but it faces challenges from neuroscience, which has shown that mental states are closely correlated with brain activity. Materialism, the view that the mind is a product of the brain and that there is only one kind of substance, matter, is the dominant view in neuroscience today. However, materialism still faces challenges in explaining the subjective nature of conscious experience. The development of new neurophilosophical approaches, which integrate insights from neuroscience, philosophy, and psychology, may be necessary to resolve the mind-brain problem. The field of neuroethics is emerging as an important area of study, addressing the ethical, legal, and social implications of neuroscience research and its applications. Neuroethics encompasses a wide range of topics, including the ethics of brain interventions, the responsible use of neurotechnologies, and the impact of neuroscience on our understanding of free will, moral responsibility, and personal identity. The use of neurotechnologies to enhance cognitive abilities, such as memory and attention, raises ethical questions about fairness, access, and the potential for creating a \"cognitive divide.\" The development of brain-reading technologies, which can decode mental states from brain activity, raises concerns about privacy and the potential for misuse. The debate over free will and moral responsibility has been reinvigorated by neuroscience research, which has shown that brain activity precedes conscious decisions. Some neuroscientists argue that this finding undermines the traditional concept of free will, while others maintain that free will is compatible with our current understanding of the brain. The implications of neuroscience for our understanding of personal identity are also being explored. Studies of patients with brain damage or neurodegenerative diseases have shown that changes in brain function can lead to profound changes in personality and self-awareness. This raises questions about the nature of the self and its relationship to the brain. The future of neuroscience will undoubtedly be shaped by interdisciplinary collaborations, bringing together researchers from diverse fields such as biology, psychology, computer science, mathematics, physics, and engineering. The development of new tools and techniques, the sharing of data and resources, and the open exchange of ideas will be essential for accelerating progress in this exciting and rapidly evolving field. The ultimate goal of neuroscience is to understand the brain in its entirety, from its molecular components to its complex functions, and to use this knowledge to improve human health and well-being.\n",
    "\"\"\"\n",
    "\n",
    "%time longt5.infer(scientific_paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T15:48:08.624659Z",
     "iopub.status.busy": "2025-04-30T15:48:08.624310Z",
     "iopub.status.idle": "2025-04-30T16:31:10.065884Z",
     "shell.execute_reply": "2025-04-30T16:31:10.064312Z",
     "shell.execute_reply.started": "2025-04-30T15:48:08.624628Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "longt5.evaluate(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "arxiv = load_dataset(\n",
    "    \"armanc/scientific_papers\",\n",
    "    \"arxiv\",\n",
    "    split=\"test\",\n",
    "    streaming=True,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "\n",
    "def convert_ARXIV_dataset(hf_dataset, X):\n",
    "    hf_dataset = hf_dataset.take(X)\n",
    "    texts, summaries = [], []\n",
    "    for sample in hf_dataset:\n",
    "        texts.append(sample[\"article\"])\n",
    "        summaries.append(sample[\"abstract\"])\n",
    "    return Dataset.from_dict(\n",
    "        {\n",
    "            \"text\": texts,\n",
    "            \"summary\": summaries,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longt5.evaluate(convert_ARXIV_dataset(arxiv, 50))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 1568668,
     "sourceId": 2582481,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "LED",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
