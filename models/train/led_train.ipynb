{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T14:05:24.169779Z",
     "iopub.status.busy": "2025-04-28T14:05:24.169302Z",
     "iopub.status.idle": "2025-04-28T14:07:01.762894Z",
     "shell.execute_reply": "2025-04-28T14:07:01.762010Z",
     "shell.execute_reply.started": "2025-04-28T14:05:24.169752Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "!pip install -q triton bitsandbytes accelerate hf_xet\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T14:07:01.764864Z",
     "iopub.status.busy": "2025-04-28T14:07:01.764625Z",
     "iopub.status.idle": "2025-04-28T14:07:26.837847Z",
     "shell.execute_reply": "2025-04-28T14:07:26.837272Z",
     "shell.execute_reply.started": "2025-04-28T14:07:01.764845Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import gc\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    BitsAndBytesConfig,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "from huggingface_hub import login\n",
    "\n",
    "import torch.utils.checkpoint\n",
    "torch.utils.checkpoint.use_reentrant = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T14:07:27.653151Z",
     "iopub.status.busy": "2025-04-28T14:07:27.652931Z",
     "iopub.status.idle": "2025-04-28T14:07:27.659479Z",
     "shell.execute_reply": "2025-04-28T14:07:27.658663Z",
     "shell.execute_reply.started": "2025-04-28T14:07:27.653133Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "BASE_MODEL = \"allenai/led-base-16384\"\n",
    "REPO_NAME = \"Mels22/led-scisummnet\"\n",
    "DATA_CSV = \"/kaggle/input/scisummnet-corpus/scisumm.csv\"\n",
    "LOCAL_DIR = \"./checkpoint\"\n",
    "\n",
    "CHUNK_SIZE = 8192\n",
    "OVERLAP_SIZE = 512\n",
    "MAX_TARGET_LENGTH = 512\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "EVAL_BATCH_SIZE = 4\n",
    "GRADIENT_ACCUMULATION = 4\n",
    "\n",
    "LR = 5e-4\n",
    "HF_TOKEN = \"YOUR_HUGGINGFACE_TOKEN\"  # Replace with your Hugging Face token\n",
    "login(token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T14:07:27.660735Z",
     "iopub.status.busy": "2025-04-28T14:07:27.660378Z",
     "iopub.status.idle": "2025-04-28T14:07:27.678814Z",
     "shell.execute_reply": "2025-04-28T14:07:27.678193Z",
     "shell.execute_reply.started": "2025-04-28T14:07:27.660710Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ScisummnetDataset:\n",
    "    def __init__(self, path, tokenizer, chunk_size=CHUNK_SIZE, overlap=OVERLAP_SIZE):\n",
    "        df = pd.read_csv(path)\n",
    "        self.hf_dataset = Dataset.from_pandas(df)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.chunk_size = chunk_size\n",
    "        self.overlap = overlap\n",
    "\n",
    "    def _process_data_to_model_inputs(self, batch):\n",
    "        all_input_ids = []\n",
    "        all_attention_masks = []\n",
    "        all_global_attention_masks = []\n",
    "        all_labels = []\n",
    "\n",
    "        for text, summary in zip(batch[\"text\"], batch[\"summary\"]):\n",
    "            tokenized_inputs = self.tokenizer(\n",
    "                text,\n",
    "                return_overflowing_tokens=True,\n",
    "                stride=self.overlap,\n",
    "                truncation=True,\n",
    "                max_length=self.chunk_size,\n",
    "                padding=\"max_length\",\n",
    "            )\n",
    "\n",
    "            tokenized_outputs = self.tokenizer(\n",
    "                summary,\n",
    "                truncation=True,\n",
    "                max_length=MAX_TARGET_LENGTH,\n",
    "                padding=\"max_length\",\n",
    "            )\n",
    "\n",
    "            for input_ids, attention_mask in zip(\n",
    "                tokenized_inputs[\"input_ids\"], tokenized_inputs[\"attention_mask\"]\n",
    "            ):\n",
    "                global_attention_mask = [0] * len(input_ids)\n",
    "                global_attention_mask[0] = 1\n",
    "\n",
    "                # Apply -100 masking to pad tokens in the label\n",
    "                labels = [\n",
    "                    -100 if token == self.tokenizer.pad_token_id else token\n",
    "                    for token in tokenized_outputs[\"input_ids\"]\n",
    "                ]\n",
    "\n",
    "                all_input_ids.append(input_ids)\n",
    "                all_attention_masks.append(attention_mask)\n",
    "                all_global_attention_masks.append(global_attention_mask)\n",
    "                all_labels.append(labels)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": np.array(all_input_ids, dtype=np.int64),\n",
    "            \"attention_mask\": np.array(all_attention_masks, dtype=np.int64),\n",
    "            \"global_attention_mask\": np.array(\n",
    "                all_global_attention_masks, dtype=np.int64\n",
    "            ),\n",
    "            \"labels\": np.array(all_labels, dtype=np.int64),\n",
    "        }\n",
    "\n",
    "    def get_data(self, test_size=0.1):\n",
    "        split_data = self.hf_dataset.train_test_split(test_size=test_size)\n",
    "        train_ds = split_data[\"train\"]\n",
    "        val_ds = split_data[\"test\"]\n",
    "\n",
    "        train_data = train_ds.map(\n",
    "            self._process_data_to_model_inputs,\n",
    "            batched=True,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            remove_columns=[\"text\", \"summary\"],\n",
    "        )\n",
    "        train_data.set_format(\n",
    "            type=\"torch\",\n",
    "            columns=[\"input_ids\", \"attention_mask\", \"global_attention_mask\", \"labels\"],\n",
    "            output_all_columns=False,  # make sure only required tensors are kept\n",
    "        )\n",
    "\n",
    "        val_data = val_ds.map(\n",
    "            self._process_data_to_model_inputs,\n",
    "            batched=True,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            remove_columns=[\"text\", \"summary\"],\n",
    "        )\n",
    "        val_data.set_format(\n",
    "            type=\"torch\",\n",
    "            columns=[\"input_ids\", \"attention_mask\", \"global_attention_mask\", \"labels\"],\n",
    "            output_all_columns=False,  # make sure only required tensors are kept\n",
    "        )\n",
    "\n",
    "        return {\"train\": train_data, \"val\": val_data}, val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T14:07:27.679540Z",
     "iopub.status.busy": "2025-04-28T14:07:27.679381Z",
     "iopub.status.idle": "2025-04-28T14:07:27.693804Z",
     "shell.execute_reply": "2025-04-28T14:07:27.693137Z",
     "shell.execute_reply.started": "2025-04-28T14:07:27.679527Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class LEDModel:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.model_checkpoint_dir = None\n",
    "        self.resume_training = None\n",
    "        self.data_collator = None\n",
    "        self.bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "        )\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            BASE_MODEL,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "            gradient_checkpointing=True,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            quantization_config=self.bnb_config,\n",
    "        )\n",
    "        lora_config = LoraConfig(\n",
    "            use_dora=True,\n",
    "            r=8,\n",
    "            lora_alpha=16,\n",
    "            lora_dropout=0.05,\n",
    "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\"],\n",
    "            bias=\"none\",\n",
    "            task_type=\"SEQ_2_SEQ_LM\",\n",
    "        )\n",
    "        self.model = prepare_model_for_kbit_training(base_model)\n",
    "        self.model = get_peft_model(self.model, lora_config)\n",
    "        print(f\"ℹ️ Initialized new model.\\n\")\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "        self.data_collator = DataCollatorForSeq2Seq(self.tokenizer, model=self.model)\n",
    "\n",
    "    def train(self, train_data, epochs, lr=LR, commit_message=\"Done training\"):\n",
    "        if self.model is None:\n",
    "            raise\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        self.model.train()\n",
    "        training_args = Seq2SeqTrainingArguments(\n",
    "            num_train_epochs=epochs,\n",
    "            output_dir=LOCAL_DIR,\n",
    "            learning_rate=lr,\n",
    "            save_strategy=\"epoch\",\n",
    "            save_total_limit=1,\n",
    "            weight_decay=0.01,\n",
    "            optim=\"paged_adamw_8bit\",\n",
    "            lr_scheduler_type=\"cosine\",\n",
    "            warmup_ratio=0.1,\n",
    "            bf16=torch.cuda.is_bf16_supported(),\n",
    "            label_names=[\"labels\"],\n",
    "            per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "            gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
    "            report_to=\"none\",\n",
    "            logging_steps=0.1,\n",
    "            push_to_hub=True,\n",
    "            hub_model_id=REPO_NAME,\n",
    "            hub_strategy=\"checkpoint\",\n",
    "        )\n",
    "\n",
    "        trainer = Seq2SeqTrainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_data[\"train\"],\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "        trainer.push_to_hub(commit_message)\n",
    "        print(f\"Pushed to HUB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "led = LEDModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T14:07:37.979884Z",
     "iopub.status.busy": "2025-04-28T14:07:37.979643Z",
     "iopub.status.idle": "2025-04-28T14:08:02.137551Z",
     "shell.execute_reply": "2025-04-28T14:08:02.136660Z",
     "shell.execute_reply.started": "2025-04-28T14:07:37.979869Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "scisummnet = ScisummnetDataset(DATA_CSV, led.tokenizer)\n",
    "data_loader, val_df = scisummnet.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "led.train(data_loader, epochs=6)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 1568668,
     "sourceId": 2582481,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
